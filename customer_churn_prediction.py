# -*- coding: utf-8 -*-
"""Customer_Churn_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QVv109SPUbyJvyjAzlosBICBdhcH8-FZ
"""

# Commented out IPython magic to ensure Python compatibility.
# Start Python Imports
import math, time, random, datetime
 
# Importing basic libraries
import numpy as np 
import pandas as pd 
 
# Input data files are downloaded and are available at my local disk
import os
from PIL import  Image
# %matplotlib inline
import itertools
import warnings
warnings.filterwarnings("ignore")
 
# Preprocessing
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold 
from sklearn import model_selection, metrics
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV

# Importing classifiers
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from mlxtend.classifier import EnsembleVoteClassifier
 
# Imports for visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls
import plotly.figure_factory as ff
 
# Evaluation
from sklearn.model_selection import learning_curve
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.metrics import roc_auc_score,roc_curve,scorer, f1_score, auc, precision_score, recall_score
import statsmodels.api as sm
from yellowbrick.classifier import DiscriminationThreshold
from sklearn.metrics import precision_recall_fscore_support

"""1. Data"""

# Import train data
from google.colab import files
uploaded = files.upload() 
import io
train = pd.read_csv(io.BytesIO(uploaded['train.csv']))

# Import test data
from google.colab import files
uploaded = files.upload() 
import io
test = pd.read_csv(io.BytesIO(uploaded['test.csv']))

# Import sample submission (how submission should look like?)
from google.colab import files
uploaded = files.upload() 
import io
sampleSubmission = pd.read_csv(io.BytesIO(uploaded['sampleSubmission.csv']))

"""2. Data Overview"""

# View the training data
train.head()

# View the testing data
test.head()

# View the example submisison dataframe
sampleSubmission.head()

train.describe()

print("Shape of the dataset: ", train.shape)
print ("\nMissing values : ", train.isnull().sum().values.sum())
print ("\nUnique values : \n", train.nunique())

#Separating churn and non churn customers
churn     = train[train["churn"] == "yes"]
not_churn = train[train["churn"] == "no"]

#Separating catagorical and numerical columns
numerical_vars  = ['account_length', 'number_vmail_messages', 'total_day_minutes', 
                   'total_day_calls', 'total_day_charge', 'total_eve_minutes', 
                   'total_eve_calls', 'total_eve_charge', 'total_night_minutes',
                   'total_night_calls',	'total_night_charge', 'total_intl_minutes',
                   'total_intl_calls', 'total_intl_charge',	'number_customer_service_calls']
categorical_vars = ['international_plan', 'voice_mail_plan', 'state', 'area_code']
train = train[['churn'] + numerical_vars + categorical_vars]
test = test[['id'] + numerical_vars + categorical_vars]

"""3. Exploratory Data Analysis"""

# Target Feature: churn
# Key: 0 = not_churn, 1 = churned

# BarPlot for churn
y = train["churn"].value_counts()
sns.barplot(y.index, y.values)

# Checking for imbalance
y_True = train["churn"][train["churn"] == 'yes']
print ("Churn Percentage = "+str( (y_True.shape[0] / train["churn"].shape[0]) * 100 ))

# In order to visualize graphs in colab with plotly, we should use the following function(custom initialization)
def configure_plotly_browser_state():
  import IPython
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
        <script>
          requirejs.config({
            paths: {
              base: '/static/base',
              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',
            },
          });
        </script>
        '''))

def plot_pie(column) :
    
    trace1 = go.Pie(values  = churn[column].value_counts().values.tolist(),
                    labels  = churn[column].value_counts().keys().tolist(),
                    hoverinfo = "label+percent+name",
                    domain  = dict(x = [0,.48]),
                    name    = "Churn Customers",
                    marker  = dict(line = dict(width = 2,
                                               color = "rgb(243,243,243)")
                                  ),
                    hole    = .6
                   )
    trace2 = go.Pie(values  = not_churn[column].value_counts().values.tolist(),
                    labels  = not_churn[column].value_counts().keys().tolist(),
                    hoverinfo = "label+percent+name",
                    marker  = dict(line = dict(width = 2,
                                               color = "rgb(243,243,243)")
                                  ),
                    domain  = dict(x = [.52,1]),
                    hole    = .6,
                    name    = "Non churn customers" 
                   )


    layout = go.Layout(dict(title = column + " donat chart for customer attrition ",
                            plot_bgcolor  = "rgb(243,243,243)",
                            paper_bgcolor = "rgb(243,243,243)",
                            annotations = [dict(text = "churn customers",
                                                font = dict(size = 13),
                                                showarrow = False,
                                                x = .15, y = .5),
                                           dict(text = "Non churn customers",
                                                font = dict(size = 13),
                                                showarrow = False,
                                                x = .88,y = .5
                                               )
                                          ]
                           )
                      )
    data = [trace1,trace2]
    fig  = go.Figure(data = data,layout = layout)
    py.iplot(fig)

# Visualizing catagorical features regarding the churn value
configure_plotly_browser_state()
plot_pie("international_plan")
plot_pie("area_code")
plot_pie("voice_mail_plan")

#function  for histogram for customer attrition types
def histogram(column) :
    trace1 = go.Histogram(x  = churn[column],
                          histnorm= "percent",
                          name = "Churn customers",
                          marker = dict(line = dict(width = .5,
                                                    color = "black"
                                                    )
                                        ),
                         opacity = .9 
                         ) 
    
    trace2 = go.Histogram(x  = not_churn[column],
                          histnorm = "percent",
                          name = "Non-churn customers",
                          marker = dict(line = dict(width = .5,
                                              color = "black"
                                             )
                                 ),
                          opacity = .9
                         )
    
    data = [trace1,trace2]
    layout = go.Layout(dict(title =column + " distribution in customer attrition ",
                            plot_bgcolor  = "rgb(243,243,243)",
                            paper_bgcolor = "rgb(243,243,243)",
                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = column,
                                             zerolinewidth=1,
                                             ticklen=5,
                                         gridwidth=2
                                            ),
                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',
                                             title = "percent",
                                             zerolinewidth=1,
                                             ticklen=5,
                                             gridwidth=2
                                            ),
                           )
                      )
    fig  = go.Figure(data=data,layout=layout)
    
    py.iplot(fig)

# Visualizing numerical features regarding the churn value
configure_plotly_browser_state()
for features in numerical_vars:
  histogram(features)

"""4. Data preprocessing"""

# Creating a new dataframe, one to host discretised continuous variables and continuous variables

df_train = pd.DataFrame() # discretised continuous variables

# Pass the values from train to the df_train 
df_train = train

# 0 and 1 for binary attributes

# Binarize churn 
df_train['churn'] = train['churn']
df_train['churn'] = np.where(df_train['churn'] == 'yes', 1, 0) # change churn to 0 for no and 1 for yes

# Binarize international_plan
df_train['international_plan'] = train['international_plan']
df_train['international_plan'] = np.where(df_train['international_plan'] == 'yes', 1, 0) 

# Binarize voice_mail_plan
df_train['voice_mail_plan'] = train['voice_mail_plan']
df_train['voice_mail_plan'] = np.where(df_train['voice_mail_plan'] == 'yes', 1, 0)

# Creating 3 new columns to host the totals of "minutes", "calls" and "charges" 

df_train['total_minutes'] = train.total_day_minutes + train.total_eve_minutes + train.total_night_minutes + train.total_intl_minutes
df_train['total_calls'] = train.total_day_calls + train.total_eve_calls + train.total_night_calls + train.total_intl_calls
df_train['total_charge'] = train.total_day_charge + train.total_eve_charge + train.total_night_charge + train.total_intl_charge

df_train

# Using dummies for area_code and state

# One hot encode the area_code column
df_area_code_one_hot = pd.get_dummies(df_train['area_code'], prefix='area_code')

# One hot encode the state column
df_state_one_hot = pd.get_dummies(df_train['state'], prefix='state')

# Combine the one hot encoded columns with df_train                                    
df_train = pd.concat([df_train, df_area_code_one_hot, df_state_one_hot], axis=1)

# Drop the original categorical columns (because now they've been one hot encoded)
df_train = df_train.drop(['area_code', 'state'], axis=1)

# Printing the df_train
df_train

# Providing a correlation Matrix for features

import plotly.graph_objects as go
from plotly.offline import init_notebook_mode,iplot
configure_plotly_browser_state()

#correlation
correlation = df_train.corr()
#tick labels
matrix_cols = correlation.columns.tolist()
#convert to array
corr_array  = np.array(correlation)
#Plotting
trace = go.Heatmap(z = corr_array,
                   x = matrix_cols,
                   y = matrix_cols,
                   colorscale = "Magma",
                   colorbar   = dict(title = "Pearson Correlation coefficient",
                                     titleside = "right"
                                    ) ,
                  )
layout = go.Layout(dict(title = "Correlation Matrix for variables",
                        autosize = False,
                        height  = 720,
                        width   = 800,
                        margin  = dict(r = 0 ,l = 210,
                                       t = 25,b = 210,
                                      ),
                        yaxis   = dict(tickfont = dict(size = 9)),
                        xaxis   = dict(tickfont = dict(size = 9))
                       )
                  )
data = [trace]
fig = go.Figure(data=data,layout=layout)
iplot(fig)

"""* The feature selection is based on the correlation matrix (removing the features with very high correlation), the feature importance provided later, as well as the evualuation metrics of the different classifiers. """

# Viewing the dataframe
df_train # Consists of 4250 rows and 27 columns

# Dropping the undesired columns from the dataframe
df_train = df_train.drop(['total_day_charge', 'total_eve_minutes', 'total_night_charge', 'total_intl_charge', 'account_length'], axis = 1)

# Selecting the dataframe we want to use first for predictions
selected_df = df_train
selected_df

# Spliting the dataframe into data and labels
X_train = selected_df.drop('churn', axis=1) # data
y_train = selected_df.churn # labels

# Shape of the data and labels
print("X_train shape: ", X_train.shape)
print("\ny_train shape: ", y_train.shape)

"""5. Building Machine Learning Models"""

# Function that runs the requested algorithm and returns the accuracy metrics
def fit_ml_algo(algo, X_train, y_train):

    skfold = StratifiedKFold(n_splits=10)
    # One Pass
    model = algo.fit(X_train, y_train)
    acc = round(model.score(X_train, y_train) * 100, 2)
    
    # Cross Validation using stratified kfold
    train_pred = model_selection.cross_val_predict(algo, X_train, y_train, cv=skfold, n_jobs = -1)

    # Cross-validation accuracy metric
    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)
    
    return train_pred, acc, acc_cv

# XGBClassifier
start_time = time.time()
XGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0.01, learning_rate=0.1, max_delta_step=0,max_depth=7,
                    min_child_weight=5, missing=None, n_estimators=20,n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,reg_alpha=0, 
                    reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1).fit(X_train, y_train)

train_pred_gbt, acc_gbt, acc_cv_gbt = fit_ml_algo(XGB, X_train, y_train)

xgb_time = (time.time() - start_time)

print("Accuracy: %s" % acc_gbt)
print("Accuracy CV 10-Fold: %s" % acc_cv_gbt)
print("Running Time: %s" % datetime.timedelta(seconds=xgb_time))

# Random forest classifier
start_time = time.time()
Random_Forest = RandomForestClassifier()

train_pred_gbt, acc_gbt, acc_cv_gbt = fit_ml_algo(Random_Forest, X_train, y_train)

rft_time = (time.time() - start_time)


print("Accuracy: %s" % acc_gbt)
print("Accuracy CV 10-Fold: %s" % acc_cv_gbt)
print("Running Time: %s" % datetime.timedelta(seconds=rft_time))

# Gradient Boosting Trees
start_time = time.time()
GBC = GradientBoostingClassifier()

train_pred_gbt, acc_gbt, acc_cv_gbt = fit_ml_algo(GBC, X_train, y_train)

gbt_time = (time.time() - start_time)

print("Accuracy: %s" % acc_gbt)
print("Accuracy CV 10-Fold: %s" % acc_cv_gbt)
print("Running Time: %s" % datetime.timedelta(seconds=gbt_time))

"""Preparing for applying Catboost"""

pip install catboost

from catboost import CatBoostClassifier, Pool, cv

# Define the categorical features for the CatBoost model
cat_features = np.where(X_train.dtypes != np.float)[0]
cat_features

# Use the CatBoost Pool() function to pool together the training data and categorical feature labels
train_pool = Pool(X_train, y_train, cat_features)

# CatBoost model definition
catboost_model = CatBoostClassifier(iterations=1000, custom_loss=['Accuracy'], loss_function='Logloss', silent=True) 
# (eval_metric='Accuracy',use_best_model=True,random_seed=42)

# Fit CatBoost model
catboost_model.fit(train_pool, plot=True)

# CatBoost accuracy
acc_catboost = round(catboost_model.score(X_train, y_train) * 100, 2)

start_time = time.time()

# Set params for cross-validation as same as initial model
cv_params = catboost_model.get_params()

# Run the cross-validation for 10-folds (same as the other models)
cv_data = cv(train_pool, cv_params, fold_count=10, plot=True)

# How long did it take?
catboost_time = (time.time() - start_time)

# CatBoost CV results save into a dataframe (cv_data), let's withdraw the maximum accuracy score
acc_cv_catboost = round(np.max(cv_data['test-Accuracy-mean']) * 100, 2)

# Print out the CatBoost model metrics
print("---CatBoost Metrics---")
print("Accuracy: {}".format(acc_catboost))
print("Accuracy cross-validation 10-Fold: {}".format(acc_cv_catboost))
print("Running Time: {}".format(datetime.timedelta(seconds=catboost_time)))

"""The following block of code contains an example of GridSearch

def best_model(model):
    print(model.best_score_)    
    print(model.best_params_)
    print(model.best_estimator_)

param_grid = {'C': [0.1,0.5,1,10,50,100], 'max_iter': [250], 'fit_intercept':[True],'intercept_scaling':[1],'penalty':['l2'], 'tol':[0.00001,0.0001,0.000001]}
log_primal_Grid = GridSearchCV(XGB,param_grid, cv=10, refit=True, verbose=0)
log_primal_Grid.fit(selected_df.loc[:, selected_df.columns != 'churn'],selected_df.churn)
best_model(log_primal_Grid)

param_grid = {'n_estimators':[150,200,250,300], 'max_depth':[15,20,25]}
log_primal_Grid = GridSearchCV(Random_Forest, param_grid, cv=10, refit=True, verbose=0)
log_primal_Grid.fit(selected_df.loc[:, selected_df.columns != 'churn'],selected_df.churn)
best_model(log_primal_Grid)

Using Ensemble Vote Classifier (Soft Voting)
"""

# Implementation of voting EnsembleVoteClassifier for classification with 10 k-fold cross validation   
from sklearn.model_selection import RepeatedStratifiedKFold
clf1 = GradientBoostingClassifier()
clf2 = RandomForestClassifier(random_state=1)
clf3 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bytree=1, gamma=0.01, learning_rate=0.1, max_delta_step=0,max_depth=7,
                    min_child_weight=5, missing=None, n_estimators=20,n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,reg_alpha=0, 
                    reg_lambda=1, scale_pos_weight=1, seed=None, silent=True, subsample=1).fit(X_train, y_train)
ensemble = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft')
 
# evaluate a give model using cross-validation
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
print('10-RepeatedStratifiedKFold cross validation:\n')
 
labels = ['GradientBoostingClassifier', 'Random Forest', 'XGBClassifier','Ensemble']
results = []
for clf, label in zip([clf1, clf2, clf3, ensemble], labels):
 
    scores = model_selection.cross_val_score(clf, X_train, y_train, cv=cv, scoring='accuracy', n_jobs=-1, error_score='raise')
    results.append(scores)
    print("Accuracy: %0.3f (+/- %0.2f) [%s]"% (scores.mean(), scores.std(), label))
 
plt.boxplot(results, labels=labels, showmeans=True)
plt.show()

"""6. Evaluation of Clasifiers

6.1 Learning Curves, Precision, Recall, F-measure, Specificity and ROCs for the classifiers
"""

# Using stratified k-fold 
skfold = StratifiedKFold(n_splits=10)

# Function for plotting several metrics
def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,
                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
    
    if axes is None:
        _, axes = plt.subplots(1, 3, figsize=(20, 5))

    axes[0].set_title(title)
    if ylim is not None:
        axes[0].set_ylim(*ylim)
    axes[0].set_xlabel("Training examples")
    axes[0].set_ylabel("Score")

    train_sizes, train_scores, test_scores, fit_times, _ = \
        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,
                       train_sizes=train_sizes,
                       return_times=True)
    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)
    fit_times_mean = np.mean(fit_times, axis=1)
    fit_times_std = np.std(fit_times, axis=1)

    thresh = 0.5

    y_pred = cross_val_predict(estimator, X, y, cv=cv)
    conf_mat = confusion_matrix(y, y_pred)

    #Metrics for evaluation
    aucc = roc_auc_score(y, y_pred)
    recall = recall_score(y, (y_pred > thresh))
    precision = precision_score(y, (y_pred > thresh))
    F1_score = 2 * precision * recall / (precision + recall)
    specificity = sum((y_pred < thresh) & (y == 0)) / sum(y == 0)


    print('Accuracy of %s: %.3f' % (title, max(test_scores_mean)))
    print('Aucc: %.3f' % (aucc))
    print('Recall: %.3f' % (recall))
    print('Precision: %.3f' % (precision))
    print('F1_score: %.3f' % (F1_score))
    print('Specificity: %.3f' % (specificity))

    
    # Plot learning curve
    axes[0].grid()
    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,
                         train_scores_mean + train_scores_std, alpha=0.1,
                         color="r")
    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,
                         test_scores_mean + test_scores_std, alpha=0.1,
                         color="g")
    axes[0].plot(train_sizes, train_scores_mean, 'o-', color="r",
                 label="Training score")
    axes[0].plot(train_sizes, test_scores_mean, 'o-', color="g",
                 label="Cross-validation score")
    axes[0].legend(loc="best")

    # Plot n_samples vs fit_times
    axes[1].grid()
    axes[1].plot(train_sizes, fit_times_mean, 'o-')
    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,
                         fit_times_mean + fit_times_std, alpha=0.1)
    axes[1].set_xlabel("Training examples")
    axes[1].set_ylabel("fit_times")
    axes[1].set_title("Scalability of the model")

    # Plot fit_time vs score
    axes[2].grid()
    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')
    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,
                         test_scores_mean + test_scores_std, alpha=0.1)
    axes[2].set_xlabel("fit_times")
    axes[2].set_ylabel("Score")
    axes[2].set_title("Performance of the model")

    fpr, tpr, thresholds = roc_curve(y, y_pred)
    roc_auc = auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(' ROC curve graph')
    plt.legend(loc="lower right")
    plt.show()

    return plt

# Calling the learning curves and printing metrics for classifiers
learncurve = plot_learning_curve(XGB, "XGBoost learning curves", X_train, y_train, cv=skfold)
learncurve = plot_learning_curve(Random_Forest, "Random Forest learning curves", X_train, y_train, cv=skfold)
learncurve = plot_learning_curve(GBC, "Gradient Boosting Classifier learning curves", X_train, y_train, cv=skfold)
learncurve = plot_learning_curve(catboost_model, "Catboost learning curves", X_train, y_train, cv=skfold)
learncurve = plot_learning_curve(ensemble, "Ensemble learning curves", X_train, y_train, cv=skfold)

"""6.2 Confusion Matrix for the classifiers"""

# Function to compute the y_pred and the confusion matrix

def conf_matrix(classifier, X_train, y_train, cv=10):
  y_pred = cross_val_predict(classifier, X_train, y_train, cv=10)
  conf_mat = confusion_matrix(y_train, y_pred)

  return y_pred, conf_mat

# Confusion Matrix with Eval metris

def plot_confusion_matrix(clf, cm, classes, normalize=False, title='Confusion Matrix',cmap=plt.cm.Oranges):
 
    print('Confusion Matrix for:', clf)
 
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
 
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
 
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

 
# Compute confusion matrixes for XGB
y_pred, XGB_matrix = conf_matrix(XGB, X_train, y_train, cv=10)
np.set_printoptions(precision=2)
 
plt.figure()
class_names = ['Not churned','Churned']
plot_confusion_matrix("XGBoost", XGB_matrix, classes=class_names, title='Confusion matrix, without normalization')
plt.show()
  
eval_metrics = classification_report(y_train, y_pred, target_names=class_names)
print(eval_metrics)
 
# Compute confusion matrixes for Random Forest
y_pred, Random_Forest_matrix = conf_matrix(Random_Forest, X_train, y_train, cv=10)
np.set_printoptions(precision=2)
 
plt.figure()
class_names = ['Not churned','Churned']
plot_confusion_matrix("Random_Forest", Random_Forest_matrix, classes=class_names, title='Confusion matrix, without normalization')
plt.show()
 
eval_metrics = classification_report(y_train, y_pred, target_names=class_names)
print(eval_metrics)
 
# Compute confusion matrixes for Gradient Boosting Classifier
y_pred, GBC_matrix = conf_matrix(GBC, X_train, y_train, cv=10)
np.set_printoptions(precision=2)
 
plt.figure()
class_names = ['Not churned','Churned']
plot_confusion_matrix("Gradient Boosting", GBC_matrix, classes=class_names, title='Confusion matrix, without normalization')
plt.show()
 
eval_metrics = classification_report(y_train, y_pred, target_names=class_names)
print(eval_metrics)
 
# Compute confusion matrixes for Catboost
y_pred, Catboost_matrix = conf_matrix(catboost_model, X_train, y_train, cv=10)
np.set_printoptions(precision=2)

plt.figure()
class_names = ['Not churned','Churned']
plot_confusion_matrix("Catboost Model", Catboost_matrix, classes=class_names, title='Confusion matrix, without normalization')
plt.show()
 
eval_metrics4 = classification_report(y_train, y_pred, target_names=class_names)
print(eval_metrics4)

# Compute confusion matrixes for Ensemble 
y_pred, ensemble_matrix = conf_matrix(ensemble, X_train, y_train, cv=10)
np.set_printoptions(precision=2)

plt.figure()
class_names = ['Not churned','Churned']
plot_confusion_matrix("ensemble Model", ensemble_matrix, classes=class_names, title='Confusion matrix, without normalization')
plt.show()
 
eval_metrics= classification_report(y_train, y_pred, target_names=class_names)
print(eval_metrics)

"""6.4 Using catboost to obtain feature importance"""

# Feature Importance based on Catboost

def feature_importance(model, data):
    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})
    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]
    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))
    return fea_imp

feature_importance(XGB, X_train)

"""7. Obtaining Results"""

# Binarize international_plan
test['international_plan'] = np.where(test['international_plan'] == 'yes', 1, 0) 

# Binarize voice_mail_plan
test['voice_mail_plan'] = np.where(test['voice_mail_plan'] == 'yes', 1, 0)

# Applying the same procedure for the test data

# One hot encode the area_code
test_area_code_one_hot = pd.get_dummies(test['area_code'], prefix='area_code')

# Combine the one hot encoded columns with df_con                                    
test = pd.concat([test, test_area_code_one_hot], axis=1)

# Drop the original categorical columns (because now they've been one hot encoded)
test = test.drop(['area_code'], axis=1) 

# One hot encode the area_code
test_state_one_hot = pd.get_dummies(test['state'], prefix='state')

# Combine the one hot encoded columns with df_con                                    
test = pd.concat([test, test_state_one_hot], axis=1)

# Drop the original categorical columns (because now they've been one hot encoded)
test = test.drop(['state'], axis=1) 

test['total_minutes'] = test.total_day_minutes + test.total_eve_minutes + test.total_night_minutes + test.total_intl_minutes
test['total_calls'] = test.total_day_calls + test.total_eve_calls + test.total_night_calls + test.total_intl_calls
test['total_charge'] = test.total_day_charge + test.total_eve_charge + test.total_night_charge + test.total_intl_charge

test.head()

# Create a list of columns to be used for the predictions
wanted_test_columns = X_train.columns
wanted_test_columns
#fit_ml_algo(ensemble, X_train, y_train)

# Make a prediction using the XBG model on the wanted columns
predictions = XGB.predict(test[wanted_test_columns])
# Our predictions array is comprised of 0's and 1's (Survived or Did Not Survive)
predictions[:20]

# Create a submisison dataframe and append the relevant columns
submission = pd.DataFrame()
submission['id'] = test['id']
submission['churn'] = predictions 
submission.head(10)

submission['churn'] = np.where(submission['churn'] == 0, 'no', 'yes')

submission.churn.value_counts()

if len(submission) == len(test):
    print("Submission dataframe is the same length as test ({} rows).".format(len(submission)))
else:
    print("Dataframes mismatched, won't be able to submit to Kaggle.")

# Convert submisison dataframe to csv for submission to csv for Kaggle submisison
from google.colab import files
submission.to_csv('catboost.csv', index=False)
files.download('catboost.csv')
print('Submission CSV is ready!')